# cli.py
import argparse
import pandas as pd
import os
import logging
import queue
import time
from datetime import datetime
from src.downloader import download_files, get_sources
from src.parser import get_downloaded_files, parse_file, extract_data, get_source_settings
from src.utils import load_excel_data, make_unique_titles, insert_dataframe_to_sql, check_cell_changes, \
    load_previous_data
from src.config import DEST_PATH, get_download_dir

# Configurer la journalisation principale
log_dir = os.path.join(os.path.dirname(__file__), "logs")
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, f"cli_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

# Formatter pour le fichier (d√©taill√©)
file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# Configurer le handler pour le fichier uniquement
file_handler = logging.FileHandler(log_file)
file_handler.setFormatter(file_formatter)

# Configurer le logger principal (pas de console handler)
logging.basicConfig(
    level=logging.INFO,
    handlers=[file_handler]
)
logger = logging.getLogger(__name__)

# Configurer un logger pour le r√©sum√©
summary_log_file = os.path.join(log_dir, f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
summary_logger = logging.getLogger('summary')
summary_logger.setLevel(logging.INFO)
summary_handler = logging.FileHandler(summary_log_file)
summary_handler.setFormatter(logging.Formatter('%(message)s'))
summary_logger.addHandler(summary_handler)

def download_and_process(args):
    """T√©l√©charge, traite et ins√®re les sources dans la BDD."""
    print("=== Phase de t√©l√©chargement ===")
    logger.info("Etape 1 : T√©l√©chargement des fichiers...")
    status_queue = queue.Queue()
    sources = get_sources()
    downloaded_sources = []
    download_errors = []
    successes, total, errors = download_files(sources, status_queue)

    # Collecter les r√©sultats du t√©l√©chargement
    for source in sources:
        print(f"--- {source} ---")
        if any(error[0] == source for error in errors):
            error_msg = next(error[1] for error in errors if error[0] == source)
            print(f"T√©l√©chargement : ‚ùå √âchec ({error_msg})")
            logger.error(f"T√©l√©chargement de {source} √©chou√© : {error_msg}")
            download_errors.append({"Source": source, "Erreur": error_msg})
        else:
            print(f"T√©l√©chargement : ‚úÖ Succ√®s")
            logger.info(f"T√©l√©chargement de {source} r√©ussi")
            downloaded_sources.append({"Source": source})
        print()  # Ajouter une ligne vide entre les blocs

    logger.info(f"T√©l√©chargements termin√©s : {successes}/{total} r√©ussis")
    print(f"\nR√©sum√© : {successes}/{total} sources t√©l√©charg√©es avec succ√®s")
    if download_errors:
        print("Sources en erreur :")
        for error in download_errors:
            print(f"- {error['Source']} ({error['Erreur']})")
    else:
        print("Sources en erreur : Aucune")

    print("\n=== Phase de traitement et insertion ===")
    logger.info("Etape 2 : Traitement et insertion dans la BDD...")
    process_and_insert(args.db_path, downloaded_sources, download_errors)

def process_only(args):
    """Traite les fichiers existants et ins√®re dans la BDD."""
    print("=== Phase de traitement et insertion ===")
    logger.info("Traitement des fichiers existants et insertion dans la BDD...")
    downloaded_sources = [{"Source": source} for source in
                          get_downloaded_files(get_download_dir(datetime.now().strftime("%m-%d"))).keys()]
    download_errors = []
    process_and_insert(args.db_path, downloaded_sources, download_errors)

def process_and_insert(db_path, downloaded_sources, download_errors):
    """Traite les fichiers et ins√®re les DataFrames dans la BDD."""
    date_str = datetime.now().strftime("%m-%d")
    downloaded_files = get_downloaded_files(get_download_dir(date_str))
    if not downloaded_files:
        logger.error("Aucun fichier t√©l√©charg√© trouv√©.")
        print("Erreur : Aucun fichier t√©l√©charg√© trouv√©.")
        return

    df_excel = load_excel_data()
    columns = df_excel.columns.tolist()

    processed_sources = []
    inserted_sources = []
    insert_errors = []
    sources_with_anomalies = []

    for source in downloaded_files:
        source_name = df_excel[df_excel[columns[0]] == source][columns[7]].iloc[0] if len(
            df_excel[df_excel[columns[0]] == source]) > 0 else source
        print(f"--- {source_name} ---")
        logger.info(f"--- {source_name} ---")

        print(f"Traitement     : ‚è≥ En cours")
        settings = get_source_settings(source)
        separator = settings.get("separator", ";")
        page = settings.get("page", 0)
        title_range = settings.get("title_range", [0, 0, 0, 5])
        data_range = settings.get("data_range", [1, 10])
        selected_table = settings.get("selected_table", None)

        if not selected_table:
            msg = f"Aucun tableau s√©lectionn√© pour {source}. Ignor√©."
            logger.warning(msg)
            print(f"Traitement     : ‚ö†Ô∏è Avertissement ({msg})")
            print(f"Insertion      : üö´ Non effectu√©")
            print(f"R√©sultat       : ‚ùå √âchec ({msg})")
            insert_errors.append({"Source": source_name, "Erreur": msg})
            print()  # Ajouter une ligne vide entre les blocs
            continue

        file_paths = downloaded_files[source]
        file_path = None
        for path in file_paths:
            if os.path.basename(path) == selected_table:
                file_path = path
                break

        if not file_path:
            msg = f"Fichier {selected_table} introuvable pour {source}. Ignor√©."
            logger.error(msg)
            print(f"Traitement     : ‚ùå √âchec ({msg})")
            print(f"Insertion      : üö´ Non effectu√©")
            print(f"R√©sultat       : ‚ùå √âchec ({msg})")
            insert_errors.append({"Source": source_name, "Erreur": msg})
            print()  # Ajouter une ligne vide entre les blocs
            continue

        try:
            download_datetime = datetime.fromtimestamp(os.path.getmtime(file_path))

            raw_data = parse_file(file_path, separator, page, selected_columns=None)
            if not raw_data:
                msg = f"Aucune donn√©e extraite pour {selected_table}. Ignor√©."
                logger.error(msg)
                print(f"Traitement     : ‚ùå √âchec ({msg})")
                print(f"Insertion      : üö´ Non effectu√©")
                print(f"R√©sultat       : ‚ùå √âchec ({msg})")
                insert_errors.append({"Source": source_name, "Erreur": msg})
                print()  # Ajouter une ligne vide entre les blocs
                continue

            titles, data = extract_data(raw_data, title_range, data_range)
            if not titles or not data:
                msg = f"Aucune donn√©e ou titre extrait pour {selected_table}. Ignor√©."
                logger.warning(msg)
                print(f"Traitement     : ‚ö†Ô∏è Avertissement ({msg})")
                print(f"Insertion      : üö´ Non effectu√©")
                print(f"R√©sultat       : ‚ùå √âchec ({msg})")
                insert_errors.append({"Source": source_name, "Erreur": msg})
                print()  # Ajouter une ligne vide entre les blocs
                continue

            data_col_count = max(len(row) for row in data) if data else 0
            if len(titles) != data_col_count:
                msg = f"Les titres ({len(titles)} colonnes) ne correspondent pas aux donn√©es ({data_col_count} colonnes)."
                logger.error(msg)
                print(f"Traitement     : ‚ùå √âchec ({msg})")
                print(f"Insertion      : üö´ Non effectu√©")
                print(f"R√©sultat       : ‚ùå √âchec ({msg})")
                insert_errors.append({"Source": source_name, "Erreur": msg})
                print()  # Ajouter une ligne vide entre les blocs
                continue

            unique_titles = make_unique_titles(titles)
            data_with_datetime = []
            for row in data:
                data_with_datetime.append([download_datetime] + row)
                time.sleep(0.001)
            unique_titles_with_datetime = ['extraction_datetime'] + unique_titles
            df_current = pd.DataFrame(data_with_datetime, columns=unique_titles_with_datetime)

            processed_sources.append({"Source": source_name})
            print(f"Traitement     : ‚úÖ Succ√®s")

            df_previous = load_previous_data(source, db_path, date_str)
            cell_anomalies = check_cell_changes(df_current, df_previous, source_name)
            anomalies_detected = bool(cell_anomalies)

            print(f"Insertion      : ‚è≥ En cours")
            table_name = source.replace(" ", "_").replace("-", "_").lower()
            try:
                insert_dataframe_to_sql(df_current, table_name, db_path)
                logger.info(f"DataFrame pour {source} ins√©r√© dans la table {table_name} avec extraction_datetime.")
                print(f"Insertion      : ‚úÖ Succ√®s")
                inserted_sources.append({"Source": source_name})
                if anomalies_detected:
                    # Capturer la premi√®re anomalie comme raison principale
                    anomaly_reason = cell_anomalies[0] if cell_anomalies else "Raison non sp√©cifi√©e"
                    logger.warning(f"Anomalie pour {source_name} : {anomaly_reason}")
                    for anomaly in cell_anomalies:
                        logger.warning(anomaly)
                    sources_with_anomalies.append({"Source": source_name, "Anomalie": anomaly_reason})
            except Exception as e:
                msg = f"Erreur lors de l'insertion de {source} dans la BDD : {str(e)}"
                logger.error(msg)
                print(f"Insertion      : ‚ùå √âchec ({str(e)[:50]}...)")
                print(f"R√©sultat       : ‚ùå √âchec ({str(e)[:50]}...)")
                insert_errors.append({"Source": source_name, "Erreur": str(e)})
                print()  # Ajouter une ligne vide entre les blocs
                continue

            result_msg = "‚úÖ Succ√®s" if not anomalies_detected else f"‚úÖ Succ√®s (avec anomalie : {anomaly_reason[:50]})"
            print(f"R√©sultat       : {result_msg}")
            print()  # Ajouter une ligne vide entre les blocs

        except Exception as e:
            msg = f"Erreur lors du traitement de {source} : {str(e)}"
            logger.error(msg)
            print(f"Traitement     : ‚ùå √âchec ({str(e)[:50]}...)")
            print(f"Insertion      : üö´ Non effectu√©")
            print(f"R√©sultat       : ‚ùå √âchec ({str(e)[:50]}...)")
            insert_errors.append({"Source": source_name, "Erreur": str(e)})
            print()  # Ajouter une ligne vide entre les blocs

    print(f"\nR√©sum√© : {len(inserted_sources)}/{len(downloaded_files)} sources ins√©r√©es avec succ√®s")
    if insert_errors:
        print("Sources en erreur :")
        for error in insert_errors:
            print(f"- {error['Source']} ({error['Erreur']})")
    else:
        print("Sources en erreur : Aucune")
    if sources_with_anomalies:
        print("Sources avec anomalies :")
        for anomaly in sources_with_anomalies:
            print(f"- {anomaly['Source']} ({anomaly['Anomalie']})")
    else:
        print("Sources avec anomalies : Aucune")

    # Nettoyer les noms des sources pour √©viter les probl√®mes d'alignement
    def clean_source_name(source):
        return source.strip()

    # Fonction pour formater les DataFrames manuellement avec alignement √† gauche
    def format_dataframe(data, columns):
        if not data:
            return "Aucun √©l√©ment"
        lines = ["\t".join(columns)]  # En-t√™te
        for item in data:
            line = "\t".join(clean_source_name(str(item.get(col, ""))) for col in columns)
            lines.append(line)
        return "\n".join(lines)

    # G√©n√©rer le fichier de log de r√©sum√© avec alignement √† gauche
    summary_logger.info("R√©sum√© de l'ex√©cution CLI\n")
    summary_logger.info("Sources t√©l√©charg√©es :")
    if downloaded_sources:
        summary_logger.info(format_dataframe(downloaded_sources, ["Source"]))
    else:
        summary_logger.info("Aucun √©l√©ment")
    summary_logger.info("\n")

    summary_logger.info("Sources non t√©l√©charg√©es en erreur :")
    if download_errors:
        summary_logger.info(format_dataframe(download_errors, ["Source", "Erreur"]))
    else:
        summary_logger.info("Aucun √©l√©ment")
    summary_logger.info("\n")

    summary_logger.info("Sources trait√©es :")
    if processed_sources:
        summary_logger.info(format_dataframe(processed_sources, ["Source"]))
    else:
        summary_logger.info("Aucun √©l√©ment")
    summary_logger.info("\n")

    summary_logger.info("Sources trait√©es ins√©r√©es dans la BDD :")
    if inserted_sources:
        summary_logger.info(format_dataframe(inserted_sources, ["Source"]))
    else:
        summary_logger.info("Aucun √©l√©ment")
    summary_logger.info("\n")

    summary_logger.info("Sources trait√©es avec anomalies :")
    if sources_with_anomalies:
        summary_logger.info(format_dataframe(sources_with_anomalies, ["Source", "Anomalie"]))
    else:
        summary_logger.info("Aucun √©l√©ment")
    summary_logger.info("\n")

    summary_logger.info("Sources trait√©es non ins√©r√©es en erreur :")
    if insert_errors:
        summary_logger.info(format_dataframe(insert_errors, ["Source", "Erreur"]))
    else:
        summary_logger.info("Aucun √©l√©ment")
    summary_logger.info("\n")

def main():
    parser = argparse.ArgumentParser(
        description="CLI pour le traitement des sources et l'insertion dans une BDD SQLite.")
    subparsers = parser.add_subparsers(dest="command")

    parser_download = subparsers.add_parser("download_and_process", help="T√©l√©charge, traite et ins√®re dans la BDD")
    parser_download.add_argument("--db_path", default="database.db", help="Chemin vers la base de donn√©es SQLite")

    parser_process = subparsers.add_parser("process_only", help="Traite les fichiers existants et ins√®re dans la BDD")
    parser_process.add_argument("--db_path", default="database.db", help="Chemin vers la base de donn√©es SQLite")

    args = parser.parse_args()

    if args.command == "download_and_process":
        download_and_process(args)
    elif args.command == "process_only":
        process_only(args)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()